{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND5YC08YdALrkajAcADHFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mauricio8583/SimpleChatbot/blob/main/ChatbotDevelopment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "#!pip install torch\n",
        "#!pip install langchain_community\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "template = \"\"\" You are a friendly chatbot engagin in a conversation with a human\n",
        "       Previous conversation:\n",
        "       {chat_history}\n",
        "\n",
        "       New human question: {question}\n",
        "       Response:\n",
        "       \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "repo_id = 'google/flan-t5-large'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(repo_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, max_length=512, temperature=0.7, top_p=0.9)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', max_memory_length=5)\n",
        "\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "while True:\n",
        "  try:\n",
        "    query = input(\"What do you wanna know: \")\n",
        "    if query.lower() == 'exit':\n",
        "      print(\"Chatbot session ended, goodbye!\")\n",
        "      break\n",
        "    print(conversation({'question': query})['text'])\n",
        "  except Exception as e:\n",
        "    print(f\"An error ocurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDdHKfkr0_dN",
        "outputId": "80eab662-be29-4e2b-8242-0350877b81f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What do you wanna know: Hi chat! How are you doing?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-831f655714b1>:38: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(conversation({'question': query})['text'])\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m You are a friendly chatbot engagin in a conversation with a human\n",
            "       Previous conversation:\n",
            "       \n",
            "\n",
            "       New human question: Hi chat! How are you doing?\n",
            "       Response:\n",
            "       \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm fine. I'm looking for a job.\n",
            "What do you wanna know: How old are you?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m You are a friendly chatbot engagin in a conversation with a human\n",
            "       Previous conversation:\n",
            "       Human: Hi chat! How are you doing?\n",
            "AI: I'm fine. I'm looking for a job.\n",
            "\n",
            "       New human question: How old are you?\n",
            "       Response:\n",
            "       \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm 25.\n",
            "What do you wanna know: Exit\n",
            "Chatbot session ended, goodbye!\n"
          ]
        }
      ]
    }
  ]
}